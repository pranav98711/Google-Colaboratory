{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Optimization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOaZPAKg/pDe8QXQTvZSWt+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pranav98711/Google-Colaboratory/blob/master/DeepLearning/MyProjects/Part4_Optimization/Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrBtLO1D9HQE",
        "colab_type": "text"
      },
      "source": [
        "# Optimization Method\n",
        "\n",
        "Until now, you've always used Gradient Descent to update the parameters and minimize the cost. Here, you will learn more advanced optimization methods that can speed up learning and perhaps even get you to a better final value for the cost function. Having a good optimization algorithm can be the difference between waiting days vs. just a few hours to get a good result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-d96xQ4skVl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io\n",
        "import math\n",
        "import sklearn \n",
        "import sklearn.datasets\n",
        "\n",
        "from sample_data.opt_utils_v1a import load_params_and_grads, initialize_parameters, forward_propagation, backward_propagation\n",
        "from sample_data.opt_utils_v1a import compute_cost, predict, predict_dec, plot_decision_boundary, load_dataset\n",
        "from sample_data.testCases import *\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (7.0 , 4.0)\n",
        "plt.rcParams['image.interpolation']='nearest'\n",
        "plt.rcParams['image.cmap'] = 'grap'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfTlT1jfuEn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3lPTkAkuKsX",
        "colab_type": "text"
      },
      "source": [
        "Mini-Batch Gradient Descent\n",
        "\n",
        "1.Shuffle     2.Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itWpUlApuMh9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_mini_batches( X,Y,mini_batch_size=64 , seed=0):\n",
        "  np.random.seed(seed)\n",
        "  m=X.shape[1]\n",
        "  mini_batches=[]\n",
        "\n",
        "  #shuffle\n",
        "  permutation=list(np.random.permutation(m))\n",
        "  shuffled_X=X[: , permutation]\n",
        "  shuffled_Y=Y[: , permutation].reshape((1,m))\n",
        "\n",
        "  #partition\n",
        "  num_complete_minibatches = math.floor(m/mini_batch_size)\n",
        "\n",
        "  for k in range(0,num_complete_minibatches):\n",
        "    mini_batch_X = shuffled_X[: , k*mini_batch_size : (k+1)*mini_batch_size]\n",
        "    mini_batch_Y = shuffled_Y[: , k*mini_batch_size : (k+1)*mini_batch_size]\n",
        "\n",
        "    mini_batch=(mini_batch_X, mini_batch_Y)\n",
        "    mini_batches.append(mini_batch)\n",
        "\n",
        "  if m%mini_batch_size != 0:\n",
        "    mini_batch_X = shuffled_X[ : , num_complete_minibatches * mini_batch_size]\n",
        "    mini_batch_Y = shuffled_Y[ : , num_complete_minibatches * mini_batch_size]\n",
        "\n",
        "    mini_batch=(mini_batch_X, mini_batch_Y)\n",
        "    mini_batches.append(mini_batch)\n",
        "\n",
        "\n",
        "  return(mini_batches)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE-swrGUwmXH",
        "colab_type": "code",
        "outputId": "066650e6-87ee-4fc5-837d-675ad3adee41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "X_assess, Y_assess, mini_batch_size = random_mini_batches_test_case()\n",
        "mini_batches = random_mini_batches(X_assess, Y_assess, mini_batch_size)\n",
        "\n",
        "print (\"shape of the 1st mini_batch_X: \" + str(mini_batches[0][0].shape))\n",
        "print (\"shape of the 2nd mini_batch_X: \" + str(mini_batches[1][0].shape))\n",
        "print (\"shape of the 3rd mini_batch_X: \" + str(mini_batches[2][0].shape))\n",
        "print (\"shape of the 1st mini_batch_Y: \" + str(mini_batches[0][1].shape))\n",
        "print (\"shape of the 2nd mini_batch_Y: \" + str(mini_batches[1][1].shape)) \n",
        "print (\"shape of the 3rd mini_batch_Y: \" + str(mini_batches[2][1].shape))\n",
        "print (\"mini batch sanity check: \" + str(mini_batches[0][0][0][0:3]))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of the 1st mini_batch_X: (12288, 64)\n",
            "shape of the 2nd mini_batch_X: (12288, 64)\n",
            "shape of the 3rd mini_batch_X: (12288,)\n",
            "shape of the 1st mini_batch_Y: (1, 64)\n",
            "shape of the 2nd mini_batch_Y: (1, 64)\n",
            "shape of the 3rd mini_batch_Y: (1,)\n",
            "mini batch sanity check: [ 0.90085595 -0.7612069   0.2344157 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUwDWQUCwwuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlv4abo71K9A",
        "colab_type": "text"
      },
      "source": [
        "# Momentum\n",
        "Because mini-batch gradient descent makes a parameter update after seeing just a subset of examples, the direction of the update has some variance, and so the path taken by mini-batch gradient descent will \"oscillate\" toward convergence. Using momentum can reduce these oscillations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jhTjLHP1MCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_velocity(parameters):\n",
        "  L=len(parameters)//2\n",
        "  v={}\n",
        "\n",
        "  for l in range(L):\n",
        "    v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
        "    v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
        "\n",
        "  return v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O9fhq_G1mYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters_with_momentum(parameters,grads, v , beta , learning_rate):\n",
        "  L = len(parameters)//2\n",
        "\n",
        "  for l in range(L):\n",
        "    #velocity\n",
        "    v[\"dW\" + str(l+1)]=beta*v['dW' + str(l+1)] + (1-beta)*grads['dW' + str(l+1)]\n",
        "    v[\"db\" + str(l+1)]=beta*v['db' + str(l+1)] + (1-beta)*grads['db' + str(l+1)]\n",
        "    #update param\n",
        "    parameters[\"W\" + str(l+1)] =  parameters[\"W\" + str(l+1)] - learning_rate*v[\"dW\" + str(l+1)] \n",
        "    parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*v[\"db\" + str(l+1)] \n",
        "\n",
        "  return parameters,v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGms7D_g2WiG",
        "colab_type": "code",
        "outputId": "fd3f7e98-2f48-462e-a0bb-21ddfc41450a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "parameters, grads, v = update_parameters_with_momentum_test_case()\n",
        "\n",
        "parameters, v = update_parameters_with_momentum(parameters, grads, v, beta = 0.9, learning_rate = 0.01)\n",
        "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \\n\" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \\n\" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \\n\" + str(parameters[\"b2\"]))\n",
        "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = v\" + str(v[\"db2\"]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = \n",
            "[[ 1.62544598 -0.61290114 -0.52907334]\n",
            " [-1.07347112  0.86450677 -2.30085497]]\n",
            "b1 = \n",
            "[[ 1.74493465]\n",
            " [-0.76027113]]\n",
            "W2 = \n",
            "[[ 0.31930698 -0.24990073  1.4627996 ]\n",
            " [-2.05974396 -0.32173003 -0.38320915]\n",
            " [ 1.13444069 -1.0998786  -0.1713109 ]]\n",
            "b2 = \n",
            "[[-0.87809283]\n",
            " [ 0.04055394]\n",
            " [ 0.58207317]]\n",
            "v[\"dW1\"] = \n",
            "[[-0.11006192  0.11447237  0.09015907]\n",
            " [ 0.05024943  0.09008559 -0.06837279]]\n",
            "v[\"db1\"] = \n",
            "[[-0.01228902]\n",
            " [-0.09357694]]\n",
            "v[\"dW2\"] = \n",
            "[[-0.02678881  0.05303555 -0.06916608]\n",
            " [-0.03967535 -0.06871727 -0.08452056]\n",
            " [-0.06712461 -0.00126646 -0.11173103]]\n",
            "v[\"db2\"] = v[[0.02344157]\n",
            " [0.16598022]\n",
            " [0.07420442]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR4KWxJD2Yxv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIhbNnLc3r7d",
        "colab_type": "text"
      },
      "source": [
        "# Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1DKdVt73uEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_adam(parameters):\n",
        "    L = len(parameters) // 2\n",
        "    v = {}\n",
        "    s = {}\n",
        "\n",
        "    for l in range(L):\n",
        "        v[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
        "        v[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
        "        s[\"dW\" + str(l+1)] = np.zeros((parameters[\"W\" + str(l+1)].shape))\n",
        "        s[\"db\" + str(l+1)] = np.zeros((parameters[\"b\" + str(l+1)].shape))\n",
        "\n",
        "    \n",
        "    return v, s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_wEVJCJ6jig",
        "colab_type": "code",
        "outputId": "1f548946-d858-4ba9-ae05-318afcfe5234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "parameters = initialize_adam_test_case()\n",
        "\n",
        "v, s = initialize_adam(parameters)\n",
        "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\n",
        "print(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\n",
        "print(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\n",
        "print(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\n",
        "print(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "v[\"dW1\"] = \n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "v[\"db1\"] = \n",
            "[[0.]\n",
            " [0.]]\n",
            "v[\"dW2\"] = \n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "v[\"db2\"] = \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]]\n",
            "s[\"dW1\"] = \n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "s[\"db1\"] = \n",
            "[[0.]\n",
            " [0.]]\n",
            "s[\"dW2\"] = \n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "s[\"db2\"] = \n",
            "[[0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qlHCpUy3x4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters_with_adam(parameters, grads , v ,s ,t ,learning_rate=0.01 , beta1=0.9 , beta2=0.999 , epsilon=1e-8):\n",
        "  L = len(parameters) //2\n",
        "  v_corrected={}\n",
        "  s_corrected={}\n",
        "\n",
        "  for l in range(L):\n",
        "    v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
        "    v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
        "\n",
        "    v_corrected[\"dW\" + str(l+1)]  = v[\"dW\" + str(l+1)] / (1 - np.power(beta1, t))\n",
        "    v_corrected[\"db\" + str(l+1)] =  v[\"db\" + str(l+1)] / (1 - np.power(beta1, t))\n",
        "\n",
        "    s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
        "    s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
        "              \n",
        "    s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "    s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
        "        \n",
        "    parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)  \n",
        "    parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n",
        "\n",
        "  return parameters , v , s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lg7xrS7y5sKZ",
        "colab_type": "code",
        "outputId": "e078d09d-6e82-4cf8-924b-208911ccd8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "parameters, grads, v, s = update_parameters_with_adam_test_case()\n",
        "parameters, v, s  = update_parameters_with_adam(parameters, grads, v, s, t = 2)\n",
        "\n",
        "print(\"W1 = \\n\" + str(parameters[\"W1\"]))\n",
        "print(\"b1 = \\n\" + str(parameters[\"b1\"]))\n",
        "print(\"W2 = \\n\" + str(parameters[\"W2\"]))\n",
        "print(\"b2 = \\n\" + str(parameters[\"b2\"]))\n",
        "print(\"v[\\\"dW1\\\"] = \\n\" + str(v[\"dW1\"]))\n",
        "print(\"v[\\\"db1\\\"] = \\n\" + str(v[\"db1\"]))\n",
        "print(\"v[\\\"dW2\\\"] = \\n\" + str(v[\"dW2\"]))\n",
        "print(\"v[\\\"db2\\\"] = \\n\" + str(v[\"db2\"]))\n",
        "print(\"s[\\\"dW1\\\"] = \\n\" + str(s[\"dW1\"]))\n",
        "print(\"s[\\\"db1\\\"] = \\n\" + str(s[\"db1\"]))\n",
        "print(\"s[\\\"dW2\\\"] = \\n\" + str(s[\"dW2\"]))\n",
        "print(\"s[\\\"db2\\\"] = \\n\" + str(s[\"db2\"]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 = \n",
            "[[ 1.63178673 -0.61919778 -0.53561312]\n",
            " [-1.08040999  0.85796626 -2.29409733]]\n",
            "b1 = \n",
            "[[ 1.75225313]\n",
            " [-0.75376553]]\n",
            "W2 = \n",
            "[[ 0.32648046 -0.25681174  1.46954931]\n",
            " [-2.05269934 -0.31497584 -0.37661299]\n",
            " [ 1.14121081 -1.09245036 -0.16498684]]\n",
            "b2 = \n",
            "[[-0.88529978]\n",
            " [ 0.03477238]\n",
            " [ 0.57537385]]\n",
            "v[\"dW1\"] = \n",
            "[[-0.11006192  0.11447237  0.09015907]\n",
            " [ 0.05024943  0.09008559 -0.06837279]]\n",
            "v[\"db1\"] = \n",
            "[[-0.01228902]\n",
            " [-0.09357694]]\n",
            "v[\"dW2\"] = \n",
            "[[-0.02678881  0.05303555 -0.06916608]\n",
            " [-0.03967535 -0.06871727 -0.08452056]\n",
            " [-0.06712461 -0.00126646 -0.11173103]]\n",
            "v[\"db2\"] = \n",
            "[[0.02344157]\n",
            " [0.16598022]\n",
            " [0.07420442]]\n",
            "s[\"dW1\"] = \n",
            "[[0.00121136 0.00131039 0.00081287]\n",
            " [0.0002525  0.00081154 0.00046748]]\n",
            "s[\"db1\"] = \n",
            "[[1.51020075e-05]\n",
            " [8.75664434e-04]]\n",
            "s[\"dW2\"] = \n",
            "[[7.17640232e-05 2.81276921e-04 4.78394595e-04]\n",
            " [1.57413361e-04 4.72206320e-04 7.14372576e-04]\n",
            " [4.50571368e-04 1.60392066e-07 1.24838242e-03]]\n",
            "s[\"db2\"] = \n",
            "[[5.49507194e-05]\n",
            " [2.75494327e-03]\n",
            " [5.50629536e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvJ5ZUa75t_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N17_Uph79kp7",
        "colab_type": "text"
      },
      "source": [
        "# Model with different Optimization Methods\n",
        "\n",
        "See the notes\n",
        "\n",
        "Summary :-\n",
        "\n",
        "Momentum usually helps, but given the small learning rate and the simplistic dataset, its impact is almost negligeable. Also, the huge oscillations you see in the cost come from the fact that some minibatches are more difficult thans others for the optimization algorithm.\n",
        "\n",
        "Adam on the other hand, clearly outperforms mini-batch gradient descent and Momentum. If you run the model for more epochs on this simple dataset, all three methods will lead to very good results. However, you've seen that Adam converges a lot faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA3gv88P6pGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}